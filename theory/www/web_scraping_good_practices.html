
Before diving into crawling a website, should be developed an understanding of the scale and structure of the website, for that, robots.txt and 
sitemaps files can help (Jarmul & Lawson, 2017). Other details could also be important.
<br><br>
<ul>
    <li><b>Robots.txt:</b> How do you know which websites are allowed or not to scrape? The answer is through the robots.txt file that most websites 
        provide (Jarmul & Lawson, 2017). The robots.txt file is part of the Robots Exclusion Protocol (REP), a group of web standards that regulate how 
        robots crawl the web, access and index content, and serve that content up to users (source: <a href="https://moz.com/learn/seo/robotstxt" target="_blank">https://moz.com/learn/seo/robotstxt</a>). To access 
        the robots.txt file, it is just simply put robots.txt after the URL to scrape and will be shown which information on the website the host 
        allows to scrape (Eswari et. al., 2022). The restrictions are just a suggestion, but good web citizens will follow them (Jarmul & Lawson, 2017). More 
        information about the robots.txt protocol is available at <a href="http://www.robotstxt.org" target="_blank">http://www.robotstxt.org</a>.
    </li>
    <li><b>Sitemaps:</b> provides links to all the web pages, helping crawlers locate the updated content without needing to crawl every web page. The 
        sitemap standard is defined at <a href="http://www.sitemaps.org/protocol.html" target="_blank">http://www.sitemaps.org/protocol.html</a>.
    </li>
    <li><b>Estimate the size of the website</b></li>
    <li><b>Identify the technology used by a website</b></li>
    <li><b>Finding the owner of a website</b></li>
</ul>
